{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import gym.spaces\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "\n",
    "def index_to_date(index):\n",
    "\n",
    "    return (start_datetime + datetime.timedelta(index)).strftime(date_format)\n",
    "\n",
    "\n",
    "def date_to_index(date_string):\n",
    "\n",
    "    return (datetime.datetime.strptime(date_string, date_format) - start_datetime).days\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\" Create a universal normalization function across close/open ratio\n",
    "\n",
    "    Args:\n",
    "        x: input of any shape\n",
    "\n",
    "    Returns: normalized data\n",
    "\n",
    "    \"\"\"\n",
    "    return (x - 1) * 100\n",
    "\n",
    "\n",
    "def sharpe(returns, freq=30, rfr=0):\n",
    "    \"\"\" Given a set of returns, calculates naive (rfr=0) sharpe (eq 28). \"\"\"\n",
    "    return (np.sqrt(freq) * np.mean(returns - rfr + eps)) / np.std(returns - rfr + eps)\n",
    "\n",
    "\n",
    "def max_drawdown(returns):\n",
    "    \"\"\" Max drawdown. See https://www.investopedia.com/terms/m/maximum-drawdown-mdd.asp \"\"\"\n",
    "    peak = returns.max()\n",
    "    trough = returns[returns.argmax():].min()\n",
    "    return (trough - peak) / (peak + eps)\n",
    "\n",
    "def p99_drawdown(returns):\n",
    "\n",
    "    peak = np.percentile(returns, 99, interpolation='nearest')\n",
    "    percidx = list(returns).index(peak)\n",
    "    trough = returns[percidx:].min()\n",
    "    return (trough - peak) / (peak + eps)\n",
    "\n",
    "def p95_drawdown(returns):\n",
    "\n",
    "    peak = np.percentile(returns, 95, interpolation='nearest')\n",
    "    percidx = list(returns).index(peak)\n",
    "    trough = returns[percidx:].min()\n",
    "    return (trough - peak) / (peak + eps)\n",
    "\n",
    "def p90_drawdown(returns):\n",
    "\n",
    "    peak = np.percentile(returns, 90, interpolation='nearest')\n",
    "    percidx = list(returns).index(peak)\n",
    "    trough = returns[percidx:].min()\n",
    "    return (trough - peak) / (peak + eps)\n",
    "\n",
    "def under_water(returns):\n",
    "    df = pd.DataFrame(returns, columns=('A',))\n",
    "    df['cummax'] = df['A'].cummax()\n",
    "    df['underwater'] = df['A'] < df['cummax']\n",
    "    total_days = df['underwater'].sum()\n",
    "    c = 0\n",
    "    list_c =[]\n",
    "    for i in df['underwater'].values:\n",
    "        if i==False:        \n",
    "            c = 0\n",
    "        if i== True:\n",
    "            c+=1\n",
    "            list_c.append(c)\n",
    "    max_days = max(list_c)\n",
    "    return total_days, max_days\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"Acts as data provider for each new episode.\"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, history, abbreviation, steps=730, window_length=50, start_idx=0, start_date=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            history: (num_stocks, timestamp, 5) open, high, low, close, volume\n",
    "            abbreviation: a list of length num_stocks with assets name\n",
    "            steps: the total number of steps to simulate, default is 2 years\n",
    "            window_length: observation window, must be less than 50\n",
    "            start_date: the date to start. Default is None and random pick one.\n",
    "                        It should be a string e.g. '2012-08-13'\n",
    "        \"\"\"\n",
    "        assert history.shape[0] == len(abbreviation), 'Number of stock is not consistent'\n",
    "        import copy\n",
    "        self.step = 0\n",
    "        self.steps = steps + 1\n",
    "        self.window_length = window_length\n",
    "        self.start_idx = start_idx\n",
    "        self.start_date = start_date\n",
    "        self._data = history.copy() \n",
    "        self.asset_names = copy.copy(abbreviation)\n",
    "        self.data = self._data\n",
    "        self.idx = np.random.randint(low = self.window_length, high = self._data.shape[1] - self.steps)\n",
    "        self.transition_matrix = 0\n",
    "        self.discrete_win_size = 0\n",
    "        self.mark_min = 0\n",
    "        self.fin_un = 0\n",
    "    def _step(self):\n",
    "\n",
    "        self.step += 1\n",
    "        obs = self.data[:, self.step:self.step + self.window_length, :].copy()\n",
    "\n",
    "        ground_truth_obs = self.data[:, self.step + self.window_length:self.step + self.window_length + 1, :].copy()\n",
    "\n",
    "        done = self.step >= self.steps\n",
    "        return obs, done, ground_truth_obs\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "       \n",
    "        if self.start_date is None:\n",
    "            self.idx = np.random.randint(\n",
    "                low = self.window_length, high = self._data.shape[1] - self.steps)\n",
    "        else:\n",
    "           \n",
    "            self.idx = date_to_index(self.start_date) - self.start_idx\n",
    "            assert self.idx >= self.window_length and self.idx <= self._data.shape[1] - self.steps, \\\n",
    "                'Invalid start date, must be window_length day after start date and simulation steps day before end date'\n",
    "\n",
    "        data = self._data[:, self.idx - self.window_length:self.idx + self.steps + 1, :4]\n",
    "        ########################################################\n",
    "        #transition_matrix\n",
    "        markov_states_raw = self._data[:, :, 3:4] / self._data[:, :, 0:1]\n",
    "        markov_states = []\n",
    "        for i in range(3, markov_states_raw.shape[1]):\n",
    "            stepData = []\n",
    "            c = np.array([markov_states_raw[:,k,0]  for k in range(i - 3, i)])\n",
    "            stepData = normalize(c.T)\n",
    "            markov_states.append(stepData)\n",
    "            \n",
    "        markov_states = np.array(markov_states)\n",
    "        \n",
    "        self.mark_min = [np.min(markov_states[:,0,:]),np.min(markov_states[:,1,:]),np.min(markov_states[:,2,:])]\n",
    "        discrete_size = 6\n",
    "        discrete_win_size_ass1 = (np.max(markov_states[:,0,:]) - np.min(markov_states[:,0,:])) /discrete_size\n",
    "        discrete_win_size_ass2 = (np.max(markov_states[:,1,:]) - np.min(markov_states[:,1,:])) /discrete_size\n",
    "        discrete_win_size_ass3 = (np.max(markov_states[:,2,:]) - np.min(markov_states[:,2,:])) /discrete_size\n",
    "        self.discrete_win_size = [discrete_win_size_ass1,discrete_win_size_ass2,discrete_win_size_ass3]\n",
    "\n",
    "        finals = []\n",
    "        for fe in markov_states:\n",
    "            ass1 = fe[0]\n",
    "            ass2 = fe[1]\n",
    "            ass3 = fe[2]\n",
    "            ass1f = []\n",
    "            ass2f = []\n",
    "            ass3f = []\n",
    "            final = []\n",
    "            for i in range(3):\n",
    "                bin1 = np.floor((ass1[i]- np.min(markov_states[:,0,:]))/discrete_win_size_ass1 )\n",
    "                bin1 = int(bin1)\n",
    "                ass1f.append(bin1)\n",
    "          \n",
    "\n",
    "                bin2 = np.floor((ass2[i]- np.min(markov_states[:,1,:]))/discrete_win_size_ass2 )\n",
    "                bin2 = int(bin2)\n",
    "                ass2f.append(bin2)\n",
    "      \n",
    "\n",
    "                bin3 = np.floor((ass3[i]- np.min(markov_states[:,2,:]))/discrete_win_size_ass3 )\n",
    "                bin3 = int(bin3)\n",
    "                ass3f.append(bin3)\n",
    "\n",
    "                fgds = [ass1f,ass2f,ass3f]\n",
    "                final.append(fgds)\n",
    "                \n",
    "            finals.append(final[-1])\n",
    "        finals = np.array(finals)\n",
    "        \n",
    "        self.fin_un = np.unique(finals,axis =0)\n",
    "\n",
    "        \n",
    "        mark1 = finals.reshape(finals.shape[0],9)\n",
    "        mark3ext = [\"{}{}{}{}{}{}{}{}{}\".format(i, j,k,l,m,o,ij,ku,gw) for i, j,k,l,m,o,ij,ku,gw in mark1]\n",
    "        df = pd.DataFrame(columns = ['state', 'next_state'])\n",
    "        for q, val in enumerate(mark3ext [:-1]): # We don't care about last state\n",
    "            df_stg = pd.DataFrame(index=[0])\n",
    "            df_stg['state'], df_stg['next_state'] = mark3ext[q], mark3ext[q+1]\n",
    "            df = pd.concat([df, df_stg], axis = 0)\n",
    "        cross_tab = pd.crosstab(df['state'], df['next_state'])\n",
    "        tabtabtab = cross_tab.div(cross_tab.sum(axis=1), axis=0)\n",
    "        transition_matrix = tabtabtab.unstack().reorder_levels(('state','next_state'))\n",
    "        self.transition_matrix = transition_matrix\n",
    "         \n",
    "        # apply augmentation?\n",
    "        self.data = data\n",
    "        \n",
    "        return self.data[:, self.step:self.step + self.window_length, :].copy(), \\\n",
    "               self.data[:, self.step + self.window_length:self.step + self.window_length + 1, :].copy()\n",
    "        \n",
    "        \n",
    "\n",
    "class PortfolioSim(object):\n",
    "    \"\"\"\n",
    "    Portfolio management sim.\n",
    "    Params:\n",
    "    - cost e.g. 0.0025 is max in Poliniex\n",
    "    Based of [Jiang 2017](https://arxiv.org/abs/1706.10059)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, asset_names=list(), steps=730, trading_cost=0.0025, time_cost=0.0):\n",
    "        self.asset_names = asset_names\n",
    "        self.cost = trading_cost\n",
    "        self.time_cost = time_cost\n",
    "        self.steps = steps\n",
    "        self.p0 = 0\n",
    "        self.infos = []\n",
    "\n",
    "    def _step(self, w1, y1):\n",
    "        \"\"\"\n",
    "        Step.\n",
    "        w1 - new action of portfolio weights - e.g. [0.1,0.9,0.0]\n",
    "        y1 - price relative vector also called return\n",
    "            e.g. [1.0, 0.9, 1.1]\n",
    "        Numbered equations are from https://arxiv.org/abs/1706.10059\n",
    "        \"\"\"\n",
    "        assert w1.shape == y1.shape, 'w1 and y1 must have the same shape'\n",
    "        assert y1[0] == 1.0, 'y1[0] must be 1'\n",
    "\n",
    "        #p0 = self.p0\n",
    "\n",
    "        dw1 = (y1 * w1) / (np.dot(y1, w1) + eps)  # (eq7) weights evolve into\n",
    "\n",
    "        mu1 = self.cost * (np.abs(dw1 - w1)).sum()  # (eq16) cost to change portfolio\n",
    "\n",
    "        assert mu1 < 1.0, 'Cost is larger than current holding'\n",
    "\n",
    "        p1 = self.p0 * (1 - mu1) * np.dot(y1, w1)  # (eq11) final portfolio value\n",
    "\n",
    "        p1 = p1 * (1 - self.time_cost)  # we can add a cost to holding\n",
    "\n",
    "        rho1 = p1 / self.p0 - 1  # rate of returns\n",
    "        r1 = np.log((p1 + eps) / (self.p0 + eps))  # log rate of return\n",
    "        reward = r1 / self.steps * 1000.  # (22) average logarithmic accumulated return\n",
    "        # remember for next step\n",
    "        self.p0 = p1\n",
    "\n",
    "        # if we run out of money, we're done (losing all the money)\n",
    "        done = p1 == 0\n",
    "\n",
    "        info = {\n",
    "            \"reward\": reward,\n",
    "            \"log_return\": r1,\n",
    "            \"portfolio_value\": p1,\n",
    "            \"return\": y1.mean(),\n",
    "            \"rate_of_return\": rho1,\n",
    "            \"weights_mean\": w1.mean(),\n",
    "            \"weights_std\": w1.std(),\n",
    "            \"cost\": mu1,\n",
    "        }\n",
    "        self.infos.append(info)\n",
    "        return reward, info, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.infos = []\n",
    "        self.p0 = 1.0\n",
    "\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    An environment for financial portfolio management.\n",
    "    Financial portfolio management is the process of constant redistribution of a fund into different\n",
    "    financial products.\n",
    "    Based on [Jiang 2017](https://arxiv.org/abs/1706.10059)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self,\n",
    "                 history,\n",
    "                 abbreviation,\n",
    "                 steps=730,  # 2 years\n",
    "                 trading_cost=0.0025,\n",
    "                 time_cost=0.00,\n",
    "                 window_length=50,\n",
    "                 start_idx=0,\n",
    "                 sample_start_date=None\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        An environment for financial portfolio management.\n",
    "        Params:\n",
    "            steps - steps in episode\n",
    "            scale - scale data and each episode (except return)\n",
    "            augment - fraction to randomly shift data by\n",
    "            trading_cost - cost of trade as a fraction\n",
    "            time_cost - cost of holding as a fraction\n",
    "            window_length - how many past observations to return\n",
    "            start_idx - The number of days from '2012-08-13' of the dataset\n",
    "            sample_start_date - The start date sampling from the history\n",
    "        \"\"\"\n",
    "        self.window_length = window_length\n",
    "        self.num_stocks = history.shape[0]\n",
    "        self.start_idx = start_idx\n",
    "        self.df_portfolio_performance = None        \n",
    "\n",
    "        self.src = DataGenerator(history, abbreviation, steps=steps, window_length=window_length, start_idx=start_idx,\n",
    "                                 start_date=sample_start_date)\n",
    "\n",
    "        self.sim = PortfolioSim(\n",
    "            asset_names=abbreviation,\n",
    "            trading_cost=trading_cost,\n",
    "            time_cost=time_cost,\n",
    "            steps=steps)\n",
    "\n",
    "        # openai gym attributes\n",
    "        # action will be the portfolio weights from 0 to 1 for each asset\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            0, 1, shape=(len(self.src.asset_names) + 1,), dtype=np.float32)  # include cash\n",
    "\n",
    "        # get the observation space from the data min and max\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(len(abbreviation), window_length,\n",
    "                                                                                 history.shape[-1]), dtype=np.float32)\n",
    "        self.infos = []\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self._step(action)\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        np.testing.assert_almost_equal(\n",
    "            action.shape,\n",
    "            (len(self.sim.asset_names) + 1,)\n",
    "        )\n",
    "\n",
    " \n",
    "        action = np.clip(action, 0, 1)\n",
    "\n",
    "        weights = action  \n",
    "        weights /= (weights.sum() + eps)\n",
    "        weights[0] += np.clip(1 - weights.sum(), 0, 1) \n",
    "\n",
    "\n",
    "        observation, done1, ground_truth_obs = self.src._step()\n",
    "\n",
    "\n",
    "        cash_observation = np.ones((1, self.window_length, observation.shape[2]))\n",
    "        observation = np.concatenate((cash_observation, observation), axis=0)\n",
    "\n",
    "        cash_ground_truth = np.ones((1, 1, ground_truth_obs.shape[2]))\n",
    "        ground_truth_obs = np.concatenate((cash_ground_truth, ground_truth_obs), axis=0)\n",
    "\n",
    "\n",
    "        close_price_vector = observation[:, -1, 3]\n",
    "        open_price_vector = observation[:, -1, 0]\n",
    "        y1 = close_price_vector / open_price_vector\n",
    "        reward, info, done2 = self.sim._step(weights, y1)\n",
    "\n",
    " \n",
    "        info['market_value'] = np.cumprod([inf[\"return\"] for inf in self.infos + [info]])[-1]\n",
    "  \n",
    "        info['date'] = index_to_date(self.start_idx + self.src.idx + self.src.step)\n",
    "        info['steps'] = self.src.step\n",
    "        info['next_obs'] = ground_truth_obs\n",
    "\n",
    "        self.infos.append(info)\n",
    "\n",
    "        return observation, reward, done1 or done2, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.infos = []\n",
    "        self.sim.reset()\n",
    "        observation, ground_truth_obs = self.src.reset()\n",
    "        cash_observation = np.ones((1, self.window_length, observation.shape[2]))\n",
    "        observation = np.concatenate((cash_observation, observation), axis=0)\n",
    "        cash_ground_truth = np.ones((1, 1, ground_truth_obs.shape[2]))\n",
    "        ground_truth_obs = np.concatenate((cash_ground_truth, ground_truth_obs), axis=0)\n",
    "        info = {}\n",
    "        info['next_obs'] = ground_truth_obs\n",
    "        return observation, info\n",
    "\n",
    "    def _render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            return\n",
    "        if mode == 'ansi':\n",
    "            pprint(self.infos[-1])\n",
    "        elif mode == 'human':\n",
    "            return self.plot()\n",
    "            \n",
    "    def render(self, mode='human', close=False):\n",
    "        return self._render(mode='human', close=False)\n",
    "\n",
    "    def plot(self):\n",
    "\n",
    "        df_info = pd.DataFrame(self.infos)\n",
    "        df_info['date'] = pd.to_datetime(df_info['date'], format='%Y-%m-%d')\n",
    "        df_info.set_index('date', inplace=True)\n",
    "        mdd = max_drawdown(df_info.rate_of_return + 1)\n",
    "        sharpe_ratio = sharpe(df_info.rate_of_return)\n",
    "        title = 'max_drawdown={: 2.2%} sharpe_ratio={: 2.4f}'.format(mdd, sharpe_ratio)       \n",
    "        df_info[[\"portfolio_value\", \"market_value\"]].plot(title=title, fig=plt.gcf(), rot=30)\n",
    "        \n",
    "\n",
    "\n",
    "class MultiActionPortfolioEnv(PortfolioEnv):\n",
    "    def __init__(self,\n",
    "                 history,\n",
    "                 abbreviation,\n",
    "                 model_names,\n",
    "                 steps=730,  # 2 years\n",
    "                 trading_cost=0.0025,\n",
    "                 time_cost=0.00,\n",
    "                 window_length=50,\n",
    "                 start_idx=0,\n",
    "                 sample_start_date=None,\n",
    "                 ):\n",
    "        super(MultiActionPortfolioEnv, self).__init__(history, abbreviation, steps, trading_cost, time_cost, window_length,\n",
    "                              start_idx, sample_start_date)\n",
    "        self.model_names = model_names\n",
    "        self.sim = [PortfolioSim(\n",
    "            asset_names=abbreviation,\n",
    "            trading_cost=trading_cost,\n",
    "            time_cost=time_cost,\n",
    "            steps=steps) for _ in range(len(self.model_names))]\n",
    "        self.infos = []\n",
    "    def _step(self, action):\n",
    "        \"\"\" Step the environment by a vector of actions\n",
    "\n",
    "        Args:\n",
    "            action: (num_models, num_stocks + 1)\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        assert action.ndim == 2, 'Action must be a two dimensional array with shape (num_models, num_stocks + 1)'\n",
    "        assert action.shape[1] == len(self.sim[0].asset_names) + 1\n",
    "        assert action.shape[0] == len(self.model_names)\n",
    " \n",
    "        action = np.clip(action, 0, 1)\n",
    "        weights = action \n",
    "        weights /= (np.sum(weights, axis=1, keepdims=True) + eps)\n",
    "\n",
    "        weights[:, 0] += np.clip(1 - np.sum(weights, axis=1), 0, 1)\n",
    "        assert ((action >= 0) * (action <= 1)).all(), 'all action values should be between 0 and 1. Not %s' % action\n",
    "        np.testing.assert_almost_equal(np.sum(weights, axis=1), np.ones(shape=(weights.shape[0])), 3,\n",
    "                                       err_msg='weights should sum to 1. action=\"%s\"' % weights)\n",
    "        observation, done1, ground_truth_obs = self.src._step()\n",
    "\n",
    "        cash_observation = np.ones((1, self.window_length, observation.shape[2]))\n",
    "        observation = np.concatenate((cash_observation, observation), axis=0)\n",
    "\n",
    "        cash_ground_truth = np.ones((1, 1, ground_truth_obs.shape[2]))\n",
    "        ground_truth_obs = np.concatenate((cash_ground_truth, ground_truth_obs), axis=0)\n",
    "\n",
    "        close_price_vector = observation[:, -1, 3]\n",
    "        open_price_vector = observation[:, -1, 0]\n",
    "        y1 = close_price_vector / open_price_vector\n",
    "\n",
    "        rewards = np.empty(shape=(weights.shape[0]))\n",
    "        info = {}\n",
    "        dones = np.empty(shape=(weights.shape[0]), dtype=bool)\n",
    "        for i in range(weights.shape[0]):\n",
    "            reward, current_info, done2 = self.sim[i]._step(weights[i], y1)\n",
    "            rewards[i] = reward\n",
    "            info[self.model_names[i]] = current_info['portfolio_value']\n",
    "            info['return'] = current_info['return']\n",
    "            dones[i] = done2\n",
    "\n",
    "        info['market_value'] = np.cumprod([inf[\"return\"] for inf in self.infos + [info]])[-1]\n",
    "        info['date'] = index_to_date(self.start_idx + self.src.idx + self.src.step)\n",
    "        info['steps'] = self.src.step\n",
    "        info['next_obs'] = ground_truth_obs\n",
    "\n",
    "        self.infos.append(info)\n",
    "\n",
    "        return observation, rewards, np.all(dones) or done1, info\n",
    "\n",
    "    def _reset(self):\n",
    "        self.infos = []\n",
    "        for sim in self.sim:\n",
    "            sim.reset()\n",
    "        observation, ground_truth_obs = self.src.reset()\n",
    "        cash_observation = np.ones((1, self.window_length, observation.shape[2]))\n",
    "        observation = np.concatenate((cash_observation, observation), axis=0)\n",
    "        cash_ground_truth = np.ones((1, 1, ground_truth_obs.shape[2]))\n",
    "        ground_truth_obs = np.concatenate((cash_ground_truth, ground_truth_obs), axis=0)\n",
    "        info = {}\n",
    "        info['next_obs'] = ground_truth_obs\n",
    "        return observation, info\n",
    "\n",
    "    def plot(self):\n",
    "        df_info = pd.DataFrame(self.infos)\n",
    "        fig=plt.gcf()\n",
    "        title = 'Trading Performance of Models'\n",
    "        df_info['date'] = pd.to_datetime(df_info['date'], format='%Y-%m-%d')\n",
    "        df_info.set_index('date', inplace=True)\n",
    "        \n",
    "        mdd = max_drawdown(df_info[self.model_names].values + 1)\n",
    "        p99dd = p99_drawdown(df_info[self.model_names].values + 1)\n",
    "        p95dd = p95_drawdown(df_info[self.model_names].values + 1)\n",
    "        p90dd = p90_drawdown(df_info[self.model_names].values + 1)\n",
    "        ttduw, mduw = under_water(df_info[self.model_names].values)\n",
    "        sharpe_ratio = sharpe(df_info[self.model_names].values)\n",
    "        per_dic = {\"MDD\" : mdd , \"99DD\" : p99dd , \"95DD\": p95dd, \"90DD\":p90dd,\"TTDUW\" : ttduw,\"MDUW\":mduw, \"SHRPR\": sharpe_ratio}\n",
    "        \n",
    "        bmdd = max_drawdown(df_info[['market_value']].values + 1)\n",
    "        bp99dd = p99_drawdown(df_info[['market_value']].values + 1)\n",
    "        bp95dd = p95_drawdown(df_info[['market_value']].values + 1)\n",
    "        bp90dd = p90_drawdown(df_info[['market_value']].values + 1)\n",
    "        bttduw, bmduw = under_water(df_info[['market_value']].values)\n",
    "        bsharpe_ratio = sharpe(df_info[['market_value']].values)\n",
    "        b_dic = {\"MDD\" : bmdd , \"99DD\" : bp99dd , \"95DD\": bp95dd, \"90DD\":bp90dd,\"TTDUW\" : bttduw,\"MDUW\":bmduw, \"SHRPR\": bsharpe_ratio}\n",
    "\n",
    "        title = 'Max drawdown={: 2.2%}  99th percentile drawdown={: 2.2%}  95th percentile drawdown={: 2.2%}  90th percentile drawdown={: 2.2%}   Sharpe Ratio={: 2.4f}'.format(mdd,p99dd,p95dd,p90dd,sharpe_ratio)       \n",
    "        df_info[self.model_names + ['market_value']].plot(title=title, fig=fig, rot=30)\n",
    "        return per_dic, b_dic, df_info[self.model_names + ['market_value']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ActorNetwork(object):\n",
    "\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "  \n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "  \n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "                                     len(self.network_params):]\n",
    "\n",
    " \n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "             for i in range(len(self.target_network_params))]\n",
    "\n",
    "\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None] + self.a_dim)\n",
    "\n",
    "\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate). \\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        raise NotImplementedError('Create actor should return (inputs, out, scaled_out)')\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        assert isinstance(state_dim, list), 'state_dim must be a list.'\n",
    "        self.s_dim = state_dim\n",
    "        assert isinstance(action_dim, list), 'action_dim must be a list.'\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "\n",
    "\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "    \n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "                                                  + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "             for i in range(len(self.target_network_params))]\n",
    "\n",
    "\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    \n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        raise NotImplementedError('Create critic should return (inputs, action, out)')\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
    "based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size, random_seed=123):\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The robust deep deterministic policy gradient model\n",
    "\"\"\"\n",
    "\n",
    "import cvxpy as cvx\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from .replay_buffer import ReplayBuffer\n",
    "from ..base_model import BaseModel\n",
    "\n",
    "\n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "def cvx_optimizer(v,f,beta):\n",
    "    ''' worst case optimization algorithm'''\n",
    "    var = cvx.Variable(len(v))\n",
    "    #objective = cvx.Minimize(var.T * v)\n",
    "    objective = cvx.Maximize(var.T * v)\n",
    "    problem = cvx.Problem(objective, [cvx.log(var).T * f >= (beta) , cvx.sum(var) == 1, var>= 0])\n",
    "    problem.solve()            \n",
    "    obj = objective.value\n",
    "\n",
    "    return obj\n",
    "    \n",
    "class DDPG(BaseModel):\n",
    "    def __init__(self, env, sess, actor, critic, actor_noise, obs_normalizer=None, action_processor=None,\n",
    "                 config_file='config/default.json',\n",
    "                 model_save_path='weights/ddpg/ddpg.ckpt', summary_path='results/ddpg/',beta=-10,lannda = 0.5):\n",
    "        with open(config_file) as f:\n",
    "            self.config = json.load(f)\n",
    "        assert self.config != None, \"Can't load config file\"\n",
    "        np.random.seed(self.config['seed'])\n",
    "        if env:\n",
    "            env.seed(self.config['seed'])\n",
    "        self.model_save_path = model_save_path\n",
    "        self.summary_path = summary_path\n",
    "        self.sess = sess\n",
    "        self.env = env\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.actor_noise = actor_noise\n",
    "        self.obs_normalizer = obs_normalizer\n",
    "        self.action_processor = action_processor\n",
    "        self.summary_ops, self.summary_vars = build_summaries()\n",
    "        self.beta = beta\n",
    "        self.lannda = lannda\n",
    "\n",
    "    def initialize(self, load_weights=True, verbose=True):\n",
    "        \"\"\" Load training history from path. To be add feature to just load weights, not training states\n",
    "\n",
    "        \"\"\"\n",
    "        if load_weights:\n",
    "            try:\n",
    "                variables = tf.global_variables()\n",
    "                param_dict = {}\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(self.sess, self.model_save_path)\n",
    "                for var in variables:\n",
    "                    var_name = var.name[:-2]\n",
    "                    if verbose:\n",
    "                        print('Loading {} from checkpoint. Name: {}'.format(var.name, var_name))\n",
    "                    param_dict[var_name] = var\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                print('Build model from scratch')\n",
    "                self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            print('Build model from scratch')\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(self, save_every_episode=1, verbose=True, debug=False):\n",
    "\n",
    "        writer = tf.summary.FileWriter(self.summary_path, self.sess.graph)\n",
    "        print('inside DDPG train')\n",
    "        self.actor.update_target_network()\n",
    "        self.critic.update_target_network()\n",
    "\n",
    "        np.random.seed(self.config['seed'])\n",
    "        num_episode = self.config['episode']\n",
    "        batch_size = self.config['batch size']\n",
    "        gamma = self.config['gamma']\n",
    "        self.buffer = ReplayBuffer(self.config['buffer size'])\n",
    "\n",
    "        # main training loop\n",
    "        for i in range(num_episode):\n",
    "            if verbose and debug:\n",
    "                print(\"Episode: \" + str(i) + \" Replay Buffer \" + str(self.buffer.count()))\n",
    "\n",
    "            previous_observation = self.env.reset()\n",
    "            transition_matrix = self.env.src.transition_matrix\n",
    "            discrete_win_size = self.env.src.discrete_win_size\n",
    "            mark_min = self.env.src.mark_min\n",
    "            all_mark_states = self.env.src.fin_un\n",
    "            if self.obs_normalizer:\n",
    "                previous_observation = self.obs_normalizer(previous_observation)\n",
    "\n",
    "            ep_reward = 0\n",
    "            ep_ave_max_q = 0\n",
    "            # keeps sampling until done\n",
    "            for j in range(self.config['max step']):\n",
    "                action = self.actor.predict(np.expand_dims(previous_observation, axis=0)).squeeze(\n",
    "                    axis=0) + self.actor_noise()\n",
    "\n",
    "                if self.action_processor:\n",
    "                    action_take = self.action_processor(action)\n",
    "                else:\n",
    "                    action_take = action\n",
    "                # step forward\n",
    "                observation, reward, done, _ = self.env.step(action_take)\n",
    "\n",
    "                if self.obs_normalizer:\n",
    "                    observation = self.obs_normalizer(observation)\n",
    "\n",
    "                # add to buffer\n",
    "                self.buffer.add(previous_observation, action, reward, done, observation)\n",
    "\n",
    "                if self.buffer.size() >= batch_size:\n",
    "     \n",
    "                    s_batch, a_batch, r_batch, t_batch, s2_batch = self.buffer.sample_batch(batch_size)\n",
    "  \n",
    "                    mark = s_batch[:,1:,:,0]\n",
    "                    finals = []\n",
    "                    for fe in mark:\n",
    "                        ass1 = fe[0]\n",
    "                        ass2 = fe[1]\n",
    "                        ass3 = fe[2]\n",
    "                        ass1f = []\n",
    "                        ass2f = []\n",
    "                        ass3f = []\n",
    "                        final = []\n",
    "                        for ik in range(3):\n",
    "                            bin1 = np.floor((ass1[ik]- mark_min[0])/discrete_win_size[0] )\n",
    "                            bin1 = int(bin1)\n",
    "                            ass1f.append(bin1)\n",
    "                            #ass1f.astype(int)\n",
    "\n",
    "                            bin2 = np.floor((ass2[ik]- mark_min[1])/discrete_win_size[1] )\n",
    "                            bin2 = int(bin2)\n",
    "                            ass2f.append(bin2)\n",
    "                            #ass2f.astype(int)\n",
    "\n",
    "                            bin3 = np.floor((ass3[ik]- mark_min[2])/discrete_win_size[2] )\n",
    "                            bin3 = int(bin3)\n",
    "                            ass3f.append(bin3)\n",
    "\n",
    "                            fgds = [ass1f,ass2f,ass3f]\n",
    "                            final.append(fgds)\n",
    "                          \n",
    "                        finals.append(final[-1])\n",
    "                    finals = np.array(finals)\n",
    "\n",
    "                    mark1 = finals.reshape((finals.shape[0],9)).astype(int)\n",
    "               \n",
    "                    mark3ext = [\"{}{}{}{}{}{}{}{}{}\".format(u,o,p,m,n,w,ou,pm,nw) for u,o,p,m,n,w,ou,pm,nw in mark1]\n",
    "        \n",
    "                    all_mark_tokened = [\"{}{}{}{}{}{}{}{}{}\".format(h,d,y,g,x,z,hd,yg,xz) for h,d,y,g,x,z,hd,yg,xz in all_mark_states.reshape((all_mark_states.shape[0],9))]\n",
    "                    \n",
    "                    beta_max = []\n",
    "                    transition_matrix_rows = []\n",
    "                    for token in mark3ext:\n",
    "                        tr_row = transition_matrix.loc[token].values\n",
    "                        n_logartim = np.where(tr_row != 0, np.log(tr_row), 0)\n",
    "           \n",
    "                        beta_max.append(sum(tr_row * n_logartim))\n",
    "                        transition_matrix_rows.append(tr_row)\n",
    "                    \n",
    "                    target_q = self.critic.predict_target(s2_batch, self.actor.predict_target(s2_batch))\n",
    "                    \n",
    "                    mark_plus = all_mark_states.reshape((all_mark_states.shape[0],all_mark_states.shape[1],all_mark_states.shape[2],1))\n",
    "                    mark_plus = np.concatenate((np.zeros((all_mark_states.shape[0],1,all_mark_states.shape[2],1)),mark_plus),axis=1)\n",
    "                    target_q_plus = self.critic.predict_target(mark_plus, self.actor.predict_target(mark_plus))\n",
    "                    q_helper_plus = dict(zip(all_mark_tokened, target_q_plus))\n",
    "                    q_helper = dict(zip(mark3ext,target_q))\n",
    "                    for tokened in all_mark_tokened:\n",
    "                        for tokener in mark3ext:\n",
    "                            if tokened == tokener:\n",
    "                                q_helper_plus[tokened] = q_helper[tokener]\n",
    "\n",
    "                    v = np.array(q_helper_plus.values()).reshape(-1)\n",
    "                    \n",
    "                    \n",
    "                    #markov_states_batch = list(map(int, raw_markov_states_batch))\n",
    "                                                             \n",
    "                    #convex_optimization\n",
    "                    \n",
    "                    #final_robust = cvx_batch_optimizer(v,transition_matrix_rows_batch)\n",
    "                    #final_robust = np.array(final_robust).reshape(-1)\n",
    "                    #print(final_robust)\n",
    "\n",
    "                    y_i = []\n",
    "\n",
    "                    landa = self.lannda/10 \n",
    "                    for k in range(batch_size):\n",
    "                        if t_batch[k]:\n",
    "                            y_i.append(r_batch[k])\n",
    "                        else:\n",
    "                            try:\n",
    "                                #print(len(transition_matrix_rows[k]))\n",
    "                                final_robust = cvx_optimizer(v,transition_matrix_rows[k],self.beta)\n",
    "                                y_i.append(r_batch[k] + landa * gamma * target_q[k] + gamma *(1-landa) * final_robust)\n",
    "                            except:\n",
    "                                y_i.append(r_batch[k] +  gamma * target_q[k])\n",
    "                                    \n",
    "                    \n",
    "                    #print(y_i)\n",
    "                    \n",
    "                 \n",
    "                    # Update the critic given the targets\n",
    "                    predicted_q_value, _ = self.critic.train(\n",
    "                        s_batch, a_batch, np.reshape(y_i, (batch_size, 1)))\n",
    "\n",
    "                    ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                    # Update the actor policy using the sampled gradient\n",
    "                    a_outs = self.actor.predict(s_batch)\n",
    "                    grads = self.critic.action_gradients(s_batch, a_outs)\n",
    "                    self.actor.train(s_batch, grads[0])\n",
    "\n",
    "                    # Update target networks\n",
    "                    self.actor.update_target_network()\n",
    "                    self.critic.update_target_network()\n",
    "\n",
    "                ep_reward += reward\n",
    "                previous_observation = observation\n",
    "\n",
    "                if done or j == self.config['max step'] - 1:\n",
    "                    summary_str = self.sess.run(self.summary_ops, feed_dict={\n",
    "                        self.summary_vars[0]: ep_reward,\n",
    "                        self.summary_vars[1]: ep_ave_max_q / float(j)\n",
    "                    })\n",
    "\n",
    "                    writer.add_summary(summary_str, i)\n",
    "                    writer.flush()\n",
    "\n",
    "                    print('Episode: {:f}, Reward: {:.2f}, Qmax: {:.4f}'.format(i, ep_reward, (ep_ave_max_q / float(j))))\n",
    "                    break\n",
    "        print('save model.')\n",
    "        self.save_model(verbose=True)\n",
    "        print('Finish.')\n",
    "\n",
    "    def predict(self, observation):\n",
    "        \"\"\" predict the next action using actor model, only used in deploy.\n",
    "            Can be used in multiple environments.\n",
    "\n",
    "        Args:\n",
    "            observation: (batch_size, num_stocks + 1, window_length)\n",
    "\n",
    "        Returns: action array with shape (batch_size, num_stocks + 1)\n",
    "\n",
    "        \"\"\"\n",
    "        if self.obs_normalizer:\n",
    "            observation = self.obs_normalizer(observation)\n",
    "        action = self.actor.predict(observation)\n",
    "        if self.action_processor:\n",
    "            action = self.action_processor(action)\n",
    "        return action\n",
    "\n",
    "    def predict_single(self, observation):\n",
    "        \"\"\" Predict the action of a single observation\n",
    "\n",
    "        Args:\n",
    "            observation: (num_stocks + 1, window_length)\n",
    "\n",
    "        Returns: a single action array with shape (num_stocks + 1,)\n",
    "\n",
    "        \"\"\"\n",
    "        if self.obs_normalizer:\n",
    "            observation = self.obs_normalizer(observation)\n",
    "        action = self.actor.predict(np.expand_dims(observation, axis=0)).squeeze(axis=0)\n",
    "        if self.action_processor:\n",
    "            action = self.action_processor(action)\n",
    "        return action\n",
    "\n",
    "    def save_model(self, verbose=False):\n",
    "        if not os.path.exists(self.model_save_path):\n",
    "            os.makedirs(self.model_save_path, exist_ok=True)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(self.sess, self.model_save_path)\n",
    "        print(\"Model saved in %s\" % model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use DDPG to train a stock trader based on a window of history price\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from model.ddpg.actor import ActorNetwork\n",
    "from model.ddpg.critic import CriticNetwork\n",
    "from model.ddpg.ddpg import DDPG\n",
    "from model.ddpg.ornstein_uhlenbeck import OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "from environment.portfolio import PortfolioEnv\n",
    "from utils.data import read_stock_history, normalize\n",
    "\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import pprint\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "def get_model_path(window_length, predictor_type, use_batch_norm,idstocks):\n",
    "    if use_batch_norm:\n",
    "        batch_norm_str = 'batch_norm'\n",
    "    else:\n",
    "        batch_norm_str = 'no_batch_norm'\n",
    "    return 'weights/stock/{}/window_{}/{}/{}/checkpoint.ckpt'.format(predictor_type, window_length, batch_norm_str,idstocks)\n",
    "\n",
    "\n",
    "def get_result_path(window_length, predictor_type, use_batch_norm,idstocks):\n",
    "    if use_batch_norm:\n",
    "        batch_norm_str = 'batch_norm'\n",
    "    else:\n",
    "        batch_norm_str = 'no_batch_norm'\n",
    "    return 'results/stock/{}/window_{}/{}/{}/'.format(predictor_type, window_length, batch_norm_str,idstocks)\n",
    "\n",
    "\n",
    "def get_variable_scope(window_length, predictor_type, use_batch_norm):\n",
    "    if use_batch_norm:\n",
    "        batch_norm_str = 'batch_norm'\n",
    "    else:\n",
    "        batch_norm_str = 'no_batch_norm'\n",
    "    return 'Robust_{}_w{}'.format(predictor_type, window_length)\n",
    "\n",
    "\n",
    "def stock_predictor(inputs, predictor_type, use_batch_norm):\n",
    "    window_length = inputs.get_shape()[2]\n",
    "    assert predictor_type in ['cnn', 'lstm'], 'type must be either cnn or lstm'\n",
    "    if predictor_type == 'cnn':\n",
    "        net = tflearn.conv_2d(inputs, 32, (1, 3), padding='valid')\n",
    "        if use_batch_norm:\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.conv_2d(net, 32, (1, window_length - 2), padding='valid')\n",
    "        if use_batch_norm:\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        if DEBUG:\n",
    "            print('After conv2d:', net.shape)\n",
    "        net = tflearn.flatten(net)\n",
    "        if DEBUG:\n",
    "            print('Output:', net.shape)\n",
    "    elif predictor_type == 'lstm':\n",
    "        num_stocks = inputs.get_shape()[1]\n",
    "        hidden_dim = 32\n",
    "        net = tflearn.reshape(inputs, new_shape=[-1, window_length, 1])\n",
    "        if DEBUG:\n",
    "            print('Reshaped input:', net.shape)\n",
    "        net = tflearn.lstm(net, hidden_dim)\n",
    "        if DEBUG:\n",
    "            print('After LSTM:', net.shape)\n",
    "        net = tflearn.reshape(net, new_shape=[-1, num_stocks, hidden_dim])\n",
    "        if DEBUG:\n",
    "            print('After reshape:', net.shape)\n",
    "        net = tflearn.flatten(net)\n",
    "        if DEBUG:\n",
    "            print('Output:', net.shape)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "class StockActor(ActorNetwork):\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size,\n",
    "                 predictor_type, use_batch_norm):\n",
    "        self.predictor_type = predictor_type\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        ActorNetwork.__init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        \"\"\"\n",
    "        self.s_dim: a list specifies shape\n",
    "        \"\"\"\n",
    "        nb_classes, window_length = self.s_dim\n",
    "        assert nb_classes == self.a_dim[0]\n",
    "        assert window_length > 2, 'This architecture only support window length larger than 2.'\n",
    "        inputs = tflearn.input_data(shape=[None] + self.s_dim + [1], name='input')\n",
    "\n",
    "        net = stock_predictor(inputs, self.predictor_type, self.use_batch_norm)\n",
    "\n",
    "        net = tflearn.fully_connected(net, 64)\n",
    "        if self.use_batch_norm:\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.fully_connected(net, 64)\n",
    "        if self.use_batch_norm:\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "\n",
    "        net = tflearn.activations.relu(net)\n",
    " \n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, self.a_dim[0], activation='softmax', weights_init=w_init)\n",
    "  \n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        window_length = self.s_dim[1]\n",
    "        inputs = inputs[:, :, -window_length:, :]\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        window_length = self.s_dim[1]\n",
    "        inputs = inputs[:, :, -window_length:, :]\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        window_length = self.s_dim[1]\n",
    "        inputs = inputs[:, :, -window_length:, :]\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "\n",
    "class StockCritic(CriticNetwork):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, num_actor_vars,\n",
    "                 predictor_type, use_batch_norm):\n",
    "        self.predictor_type = predictor_type\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        CriticNetwork.__init__(self, sess, state_dim, action_dim, learning_rate, tau, num_actor_vars)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None] + self.s_dim + [1])\n",
    "        action = tflearn.input_data(shape=[None] + self.a_dim)\n",
    "\n",
    "        net = stock_predictor(inputs, self.predictor_type, self.use_batch_norm)\n",
    "\n",
    "\n",
    "        t1 = tflearn.fully_connected(net, 64)\n",
    "        t2 = tflearn.fully_connected(action, 64)\n",
    "\n",
    "        net = tf.add(t1, t2)\n",
    "        if self.use_batch_norm:\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        window_length = self.s_dim[1]\n",
    "        inputs = inputs[:, :, -window_length:, :]\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        window_length = self.s_dim[1]\n",
    "        inputs = inputs[:, :, -window_length:, :]\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        window_length = self.s_dim[1]\n",
    "        inputs = inputs[:, :, -window_length:, :]\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        window_length = self.s_dim[1]\n",
    "        inputs = inputs[:, :, -window_length:, :]\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "\n",
    "def obs_normalizer(observation):\n",
    "    \"\"\" Preprocess observation obtained by environment\n",
    "\n",
    "    Args:\n",
    "        observation: (nb_classes, window_length, num_features) or with info\n",
    "\n",
    "    Returns: normalized\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(observation, tuple):\n",
    "        observation = observation[0]\n",
    "    # directly use close/open ratio as feature\n",
    "    observation = observation[:, :, 3:4] / observation[:, :, 0:1]\n",
    "    observation = normalize(observation)\n",
    "\n",
    "    \n",
    "    return observation\n",
    "\n",
    "\n",
    "def test_model(env, model):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model.predict_single(observation)\n",
    "        observation, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "\n",
    "def test_model_multiple(env, models):\n",
    "    observations_list = []\n",
    "    actions_list = []\n",
    "    info_list = []\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = []\n",
    "        for model in models:\n",
    "            actions.append(model.predict_single(observation))\n",
    "        actions = np.array(actions)\n",
    "        observation, _, done, info = env.step(actions)\n",
    "        observations_list.append(observation)\n",
    "        actions_list.append(actions)\n",
    "        info_list.append(info)\n",
    "    model_dic, bench_dic, df_performance = env.render()\n",
    "    return model_dic, bench_dic, observations_list, info_list, actions_list, df_performance\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    DEBUG = False\n",
    "\n",
    "    history = read_stock_history(filepath='/stock_data/history.csv')\n",
    "    history = history[2:5,:,:4]\n",
    "    tickers = read_stock_history(filepath='/stock_data/tickers.csv')\n",
    "    tickers = tickers[2:5]\n",
    "    idstocks = \"\".join(tickers)\n",
    "    target_stocks = tickers\n",
    "    num_training_time = 1095\n",
    "    window_length = 3\n",
    "    nb_classes = len(target_stocks) + 1\n",
    "\n",
    "    target_history = np.empty(shape=(len(target_stocks), num_training_time, history.shape[2]))\n",
    "    for i, stock in enumerate(target_stocks):\n",
    "        target_history[i] = history[tickers.index(stock), :num_training_time, :]\n",
    "\n",
    "    env = PortfolioEnv(target_history, target_stocks, steps=1000, window_length=window_length)\n",
    "\n",
    "    action_dim = [nb_classes]\n",
    "    state_dim = [nb_classes, window_length]\n",
    "    batch_size = 64\n",
    "    action_bound = 1.\n",
    "    tau = 1e-3\n",
    "    predictor_type = 'cnn'\n",
    "    use_batch_norm = True\n",
    "\n",
    "    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "    model_save_path = get_model_path(window_length, predictor_type, use_batch_norm,idstocks)\n",
    "    summary_path = get_result_path(window_length, predictor_type, use_batch_norm,idstocks)\n",
    "\n",
    "    variable_scope = get_variable_scope(window_length, predictor_type, use_batch_norm)\n",
    "\n",
    "    with tf.variable_scope(variable_scope):\n",
    "        sess = tf.Session()\n",
    "        actor = StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size,\n",
    "                           predictor_type, use_batch_norm)\n",
    "        critic = StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\n",
    "                             learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(),\n",
    "                             predictor_type=predictor_type, use_batch_norm=use_batch_norm)\n",
    "        ddpg_model = DDPG(env, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer,\n",
    "                          config_file='config/stock.json', model_save_path=model_save_path,\n",
    "                          summary_path=summary_path)\n",
    "        ddpg_model.initialize(load_weights=False)\n",
    "        print('calling DDPG train')\n",
    "        ddpg_model.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
